{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dab1f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdbc333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f23e4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e78aeab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Chamodi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3828e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Chamodi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c285bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d849173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85c4e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function for remove the punctuation in the documents\n",
    "def removePunctuation(file):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    no_punct = \"\"\n",
    "    for char in file:\n",
    "        if char not in  punctuations:\n",
    "            no_punct = no_punct + char\n",
    "    return no_punct\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "044b5134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open encoding = \"UTF-8\"\n",
    "#read() file is readed\n",
    "#lower() turned all the words in to lower case\n",
    "#The strip() method removes any leading (spaces at the beginning) and trailing (spaces at the end) characters (space is the default leading character to remove)\n",
    "#split()splits a string into a list. \n",
    "doc1 = removePunctuation(open(\"1\", \"r\", encoding=\"utf-8\").read().lower().strip()).split(\" \")\n",
    "doc2 = removePunctuation(open(\"2\", \"r\", encoding=\"utf-8\").read().lower().strip()).split(\" \")\n",
    "doc3 = removePunctuation(open(\"3\", \"r\", encoding=\"utf-8\").read().lower().strip()).split(\" \")\n",
    "doc4 = removePunctuation(open(\"4\", \"r\", encoding=\"utf-8\").read().lower().strip()).split(\" \")\n",
    "doc5 = removePunctuation(open(\"5\", \"r\", encoding=\"utf-8\").read().lower().strip()).split(\" \")\n",
    "#open the query file\n",
    "query = removePunctuation(open(\"Query\", \"r\", encoding=\"utf-8\").read().lower().strip()).split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "225be380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our', 'research', 'examines', 'a', 'predictive', 'machine', 'learning', 'approach', 'for', 'financial', 'news', 'articles', 'analysis', 'using', 'several', 'different', 'textual', 'representations', 'bag', 'of', 'words', 'noun', 'phrases', 'and', 'named', 'entities', 'through', 'this', 'approach', 'we', 'investigated', '9211', 'financial', 'news', 'articles', 'and', '10259042', 'stock', 'quotes', 'covering', 'the', 'sp', '500', 'stocks', 'during', 'a', 'five', 'week', 'period', 'we', 'applied', 'our', 'analysis', 'to', 'estimate', 'a', 'discrete', 'stock', 'price', 'twenty', 'minutes', 'after', 'a', 'news', 'article', 'was', 'released', 'using', 'a', 'support', 'vector', 'machine', 'svm', 'derivative', 'specially', 'tailored', 'for', 'discrete', 'numeric', 'prediction', 'and', 'models', 'containing', 'different', 'stockspecific', 'variables', 'we', 'show', 'that', 'the', 'model', 'containing', 'both', 'article', 'terms', 'and', 'stock', 'price', 'at', 'the', 'time', 'of', 'article', 'release', 'had', 'the', 'best', 'performance', 'in', 'closeness', 'to', 'the', 'actual', 'future', 'stock', 'price', 'mse', '004261', 'the', 'same', 'direction', 'of', 'price', 'movement', 'as', 'the', 'future', 'price', '571', 'directional', 'accuracy', 'and', 'the', 'highest', 'return', 'using', 'a', 'simulated', 'trading', 'engine', '206', 'return', 'we', 'further', 'investigated', 'the', 'different', 'textual', 'representations', 'and', 'found', 'that', 'a', 'proper', 'noun', 'scheme', 'performs', 'better', 'than', 'the', 'de', 'facto', 'standard', 'of', 'bag', 'of', 'words', 'in', 'all', 'three', 'metrics'] ['we', 'trained', 'a', 'large', 'deep', 'convolutional', 'neural', 'network', 'to', 'classify', 'the', '12', 'million', 'highresolution', 'images', 'in', 'the', 'imagenet', 'lsvrc2010', 'contest', 'into', 'the', '1000', 'different', 'classes', 'on', 'the', 'test', 'data', 'we', 'achieved', 'top1', 'and', 'top5', 'error', 'rates', 'of', '375', 'and', '170', 'which', 'is', 'considerably', 'better', 'than', 'the', 'previous', 'stateoftheart', 'the', 'neural', 'network', 'which', 'has', '60', 'million', 'parameters', 'and', '650000', 'neurons', 'consists', 'of', 'five', 'convolutional', 'layers', 'some', 'of', 'which', 'are', 'followed', 'by', 'maxpooling', 'layers', 'and', 'three', 'fullyconnected', 'layers', 'with', 'a', 'final', '1000way', 'softmax', 'to', 'make', 'training', 'faster', 'we', 'used', 'nonsaturating', 'neurons', 'and', 'a', 'very', 'efficient', 'gpu', 'implementation', 'of', 'the', 'convolution', 'operation', 'to', 'reduce', 'overfitting', 'in', 'the', 'fullyconnected', 'layers', 'we', 'employed', 'a', 'recentlydeveloped', 'regularization', 'method', 'called', '“dropout”', 'that', 'proved', 'to', 'be', 'very', 'effective', 'we', 'also', 'entered', 'a', 'variant', 'of', 'this', 'model', 'in', 'the', 'ilsvrc2012', 'competition', 'and', 'achieved', 'a', 'winning', 'top5', 'test', 'error', 'rate', 'of', '153', 'compared', 'to', '262', 'achieved', 'by', 'the', 'secondbest', 'entry'] ['a', 'purely', 'peertopeer', 'version', 'of', 'electronic', 'cash', 'would', 'allow', 'online', 'payments', 'to', 'be', 'sent', 'directly', 'from', 'one', 'party', 'to', 'another', 'without', 'going', 'through', 'a', 'financial', 'institution', 'digital', 'signatures', 'provide', 'part', 'of', 'the', 'solution', 'but', 'the', 'main', 'benefits', 'are', 'lost', 'if', 'a', 'trusted', 'third', 'party', 'is', 'still', 'required', 'to', 'prevent', 'doublespending', 'we', 'propose', 'a', 'solution', 'to', 'the', 'doublespending', 'problem', 'using', 'a', 'peertopeer', 'network', 'the', 'network', 'timestamps', 'transactions', 'by', 'hashing', 'them', 'into', 'an', 'ongoing', 'chain', 'of', 'hashbased', 'proofofwork', 'forming', 'a', 'record', 'that', 'cannot', 'be', 'changed', 'without', 'redoing', 'the', 'proofofwork', 'the', 'longest', 'chain', 'not', 'only', 'serves', 'as', 'proof', 'of', 'the', 'sequence', 'of', 'events', 'witnessed', 'but', 'proof', 'that', 'it', 'came', 'from', 'the', 'largest', 'pool', 'of', 'cpu', 'power', 'as', 'long', 'as', 'a', 'majority', 'of', 'cpu', 'power', 'is', 'controlled', 'by', 'nodes', 'that', 'are', 'not', 'cooperating', 'to', 'attack', 'the', 'network', 'they’ll', 'generate', 'the', 'longest', 'chain', 'and', 'outpace', 'attackers', 'the', 'network', 'itself', 'requires', 'minimal', 'structure', 'messages', 'are', 'broadcast', 'on', 'a', 'best', 'effort', 'basis', 'and', 'nodes', 'can', 'leave', 'and', 'rejoin', 'the', 'network', 'at', 'will', 'accepting', 'the', 'longest', 'proofofwork', 'chain', 'as', 'proof', 'of', 'what', 'happened', 'while', 'they', 'were', 'gone'] ['we', 'identified', 'seasonal', 'human', 'coronaviruses', 'influenza', 'viruses', 'and', 'rhinoviruses', 'in', 'exhaled', 'breath', 'and', 'coughs', 'of', 'children', 'and', 'adults', 'with', 'acute', 'respiratory', 'illness', 'surgical', 'face', 'masks', 'significantly', 'reduced', 'detection', 'of', 'influenza', 'virus', 'rna', 'in', 'respiratory', 'droplets', 'and', 'coronavirus', 'rna', 'in', 'aerosols', 'with', 'a', 'trend', 'toward', 'reduced', 'detection', 'of', 'coronavirus', 'rna', 'in', 'respiratory', 'droplets', 'our', 'results', 'indicate', 'that', 'surgical', 'face', 'masks', 'could', 'prevent', 'transmission', 'of', 'human', 'coronaviruses', 'and', 'influenza', 'viruses', 'from', 'symptomatic', 'individuals'] ['quantum', 'computers', 'promise', 'to', 'perform', 'certain', 'tasks', 'that', 'are', 'believed', 'to', 'be', 'intractable', 'to', 'classical', 'computers', 'boson', 'sampling', 'is', 'such', 'a', 'task', 'and', 'is', 'considered', 'a', 'strong', 'candidate', 'to', 'demonstrate', 'the', 'quantum', 'computational', 'advantage', 'we', 'performed', 'gaussian', 'boson', 'sampling', 'by', 'sending', '50', 'indistinguishable', 'singlemode', 'squeezed', 'states', 'into', 'a', '100mode', 'ultralowloss', 'interferometer', 'with', 'full', 'connectivity', 'and', 'random', 'matrix—the', 'whole', 'optical', 'setup', 'is', 'phaselocked—and', 'sampling', 'the', 'output', 'using', '100', 'highefficiency', 'singlephoton', 'detectors', 'the', 'obtained', 'samples', 'were', 'validated', 'against', 'plausible', 'hypotheses', 'exploiting', 'thermal', 'states', 'distinguishable', 'photons', 'and', 'uniform', 'distribution', 'the', 'photonic', 'quantum', 'computer', 'jiuzhang', 'generates', 'up', 'to', '76', 'output', 'photon', 'clicks', 'which', 'yields', 'an', 'output', 'statespace', 'dimension', 'of', '1030', 'and', 'a', 'sampling', 'rate', 'that', 'is', 'faster', 'than', 'using', 'the', 'stateoftheart', 'simulation', 'strategy', 'and', 'supercomputers', 'by', 'a', 'factor', 'of', '1014']\n"
     ]
    }
   ],
   "source": [
    "#print the split words\n",
    "print (doc1, doc2, doc3, doc4, doc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce603147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trained', 'deep', 'convolutional', 'neural', 'network', 'which', 'has', '60', 'million', 'parameters', 'and', '650000', 'neurons', 'consists', 'of', 'five', 'convolutional', 'layers', 'some', 'of', 'which', 'are', 'followed', 'by', 'maxpooling', 'layers', 'and', 'three', 'fullyconnected', 'layers', 'with', 'a', 'final', '1000way', 'softmax', 'to', 'classify', 'the', '12', 'million', 'highresolution', 'images', 'in', 'the', 'imagenet', 'lsvrc2010', 'contest', 'into', 'the', '1000', 'different', 'classes']\n"
     ]
    }
   ],
   "source": [
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55977adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the stop word from the document using the NLTK\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "760723e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered split words\n",
    "doc1_filtered_sentence = [word for word in doc1 if not word in stop_words]\n",
    "doc2_filtered_sentence = [word for word in doc2 if not word in stop_words]\n",
    "doc3_filtered_sentence = [word for word in doc3 if not word in stop_words]\n",
    "doc4_filtered_sentence = [word for word in doc4 if not word in stop_words]\n",
    "doc5_filtered_sentence = [word for word in doc5 if not word in stop_words]\n",
    "#query filtered split words\n",
    "query_filtered_sentence = [word for word in query if not word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "521f3470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['research', 'examines', 'predictive', 'machine', 'learning', 'approach', 'financial', 'news', 'articles', 'analysis', 'using', 'several', 'different', 'textual', 'representations', 'bag', 'words', 'noun', 'phrases', 'named', 'entities', 'approach', 'investigated', '9211', 'financial', 'news', 'articles', '10259042', 'stock', 'quotes', 'covering', 'sp', '500', 'stocks', 'five', 'week', 'period', 'applied', 'analysis', 'estimate', 'discrete', 'stock', 'price', 'twenty', 'minutes', 'news', 'article', 'released', 'using', 'support', 'vector', 'machine', 'svm', 'derivative', 'specially', 'tailored', 'discrete', 'numeric', 'prediction', 'models', 'containing', 'different', 'stockspecific', 'variables', 'show', 'model', 'containing', 'article', 'terms', 'stock', 'price', 'time', 'article', 'release', 'best', 'performance', 'closeness', 'actual', 'future', 'stock', 'price', 'mse', '004261', 'direction', 'price', 'movement', 'future', 'price', '571', 'directional', 'accuracy', 'highest', 'return', 'using', 'simulated', 'trading', 'engine', '206', 'return', 'investigated', 'different', 'textual', 'representations', 'found', 'proper', 'noun', 'scheme', 'performs', 'better', 'de', 'facto', 'standard', 'bag', 'words', 'three', 'metrics'] ['trained', 'large', 'deep', 'convolutional', 'neural', 'network', 'classify', '12', 'million', 'highresolution', 'images', 'imagenet', 'lsvrc2010', 'contest', '1000', 'different', 'classes', 'test', 'data', 'achieved', 'top1', 'top5', 'error', 'rates', '375', '170', 'considerably', 'better', 'previous', 'stateoftheart', 'neural', 'network', '60', 'million', 'parameters', '650000', 'neurons', 'consists', 'five', 'convolutional', 'layers', 'followed', 'maxpooling', 'layers', 'three', 'fullyconnected', 'layers', 'final', '1000way', 'softmax', 'make', 'training', 'faster', 'used', 'nonsaturating', 'neurons', 'efficient', 'gpu', 'implementation', 'convolution', 'operation', 'reduce', 'overfitting', 'fullyconnected', 'layers', 'employed', 'recentlydeveloped', 'regularization', 'method', 'called', '“dropout”', 'proved', 'effective', 'also', 'entered', 'variant', 'model', 'ilsvrc2012', 'competition', 'achieved', 'winning', 'top5', 'test', 'error', 'rate', '153', 'compared', '262', 'achieved', 'secondbest', 'entry'] ['purely', 'peertopeer', 'version', 'electronic', 'cash', 'would', 'allow', 'online', 'payments', 'sent', 'directly', 'one', 'party', 'another', 'without', 'going', 'financial', 'institution', 'digital', 'signatures', 'provide', 'part', 'solution', 'main', 'benefits', 'lost', 'trusted', 'third', 'party', 'still', 'required', 'prevent', 'doublespending', 'propose', 'solution', 'doublespending', 'problem', 'using', 'peertopeer', 'network', 'network', 'timestamps', 'transactions', 'hashing', 'ongoing', 'chain', 'hashbased', 'proofofwork', 'forming', 'record', 'cannot', 'changed', 'without', 'redoing', 'proofofwork', 'longest', 'chain', 'serves', 'proof', 'sequence', 'events', 'witnessed', 'proof', 'came', 'largest', 'pool', 'cpu', 'power', 'long', 'majority', 'cpu', 'power', 'controlled', 'nodes', 'cooperating', 'attack', 'network', 'they’ll', 'generate', 'longest', 'chain', 'outpace', 'attackers', 'network', 'requires', 'minimal', 'structure', 'messages', 'broadcast', 'best', 'effort', 'basis', 'nodes', 'leave', 'rejoin', 'network', 'accepting', 'longest', 'proofofwork', 'chain', 'proof', 'happened', 'gone'] ['identified', 'seasonal', 'human', 'coronaviruses', 'influenza', 'viruses', 'rhinoviruses', 'exhaled', 'breath', 'coughs', 'children', 'adults', 'acute', 'respiratory', 'illness', 'surgical', 'face', 'masks', 'significantly', 'reduced', 'detection', 'influenza', 'virus', 'rna', 'respiratory', 'droplets', 'coronavirus', 'rna', 'aerosols', 'trend', 'toward', 'reduced', 'detection', 'coronavirus', 'rna', 'respiratory', 'droplets', 'results', 'indicate', 'surgical', 'face', 'masks', 'could', 'prevent', 'transmission', 'human', 'coronaviruses', 'influenza', 'viruses', 'symptomatic', 'individuals'] ['quantum', 'computers', 'promise', 'perform', 'certain', 'tasks', 'believed', 'intractable', 'classical', 'computers', 'boson', 'sampling', 'task', 'considered', 'strong', 'candidate', 'demonstrate', 'quantum', 'computational', 'advantage', 'performed', 'gaussian', 'boson', 'sampling', 'sending', '50', 'indistinguishable', 'singlemode', 'squeezed', 'states', '100mode', 'ultralowloss', 'interferometer', 'full', 'connectivity', 'random', 'matrix—the', 'whole', 'optical', 'setup', 'phaselocked—and', 'sampling', 'output', 'using', '100', 'highefficiency', 'singlephoton', 'detectors', 'obtained', 'samples', 'validated', 'plausible', 'hypotheses', 'exploiting', 'thermal', 'states', 'distinguishable', 'photons', 'uniform', 'distribution', 'photonic', 'quantum', 'computer', 'jiuzhang', 'generates', '76', 'output', 'photon', 'clicks', 'yields', 'output', 'statespace', 'dimension', '1030', 'sampling', 'rate', 'faster', 'using', 'stateoftheart', 'simulation', 'strategy', 'supercomputers', 'factor', '1014']\n"
     ]
    }
   ],
   "source": [
    "print (doc1_filtered_sentence , doc2_filtered_sentence, doc3_filtered_sentence, doc4_filtered_sentence, doc5_filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0384087a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trained', 'deep', 'convolutional', 'neural', 'network', '60', 'million', 'parameters', '650000', 'neurons', 'consists', 'five', 'convolutional', 'layers', 'followed', 'maxpooling', 'layers', 'three', 'fullyconnected', 'layers', 'final', '1000way', 'softmax', 'classify', '12', 'million', 'highresolution', 'images', 'imagenet', 'lsvrc2010', 'contest', '1000', 'different', 'classes']\n"
     ]
    }
   ],
   "source": [
    "print (query_filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1d7d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The union() method returns a set that contains all items from the original set, and all items from the specified set(s).\n",
    "#If an item is present in more than one set, the result will contain only one appearance of this item.\n",
    "wordSet = set(doc1_filtered_sentence).union(set(doc2_filtered_sentence),set(doc3_filtered_sentence),set(doc4_filtered_sentence),set(doc5_filtered_sentence),set(query_filtered_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf224b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'004261',\n",
       " '100',\n",
       " '1000',\n",
       " '1000way',\n",
       " '100mode',\n",
       " '1014',\n",
       " '10259042',\n",
       " '1030',\n",
       " '12',\n",
       " '153',\n",
       " '170',\n",
       " '206',\n",
       " '262',\n",
       " '375',\n",
       " '50',\n",
       " '500',\n",
       " '571',\n",
       " '60',\n",
       " '650000',\n",
       " '76',\n",
       " '9211',\n",
       " 'accepting',\n",
       " 'accuracy',\n",
       " 'achieved',\n",
       " 'actual',\n",
       " 'acute',\n",
       " 'adults',\n",
       " 'advantage',\n",
       " 'aerosols',\n",
       " 'allow',\n",
       " 'also',\n",
       " 'analysis',\n",
       " 'another',\n",
       " 'applied',\n",
       " 'approach',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'attack',\n",
       " 'attackers',\n",
       " 'bag',\n",
       " 'basis',\n",
       " 'believed',\n",
       " 'benefits',\n",
       " 'best',\n",
       " 'better',\n",
       " 'boson',\n",
       " 'breath',\n",
       " 'broadcast',\n",
       " 'called',\n",
       " 'came',\n",
       " 'candidate',\n",
       " 'cannot',\n",
       " 'cash',\n",
       " 'certain',\n",
       " 'chain',\n",
       " 'changed',\n",
       " 'children',\n",
       " 'classes',\n",
       " 'classical',\n",
       " 'classify',\n",
       " 'clicks',\n",
       " 'closeness',\n",
       " 'compared',\n",
       " 'competition',\n",
       " 'computational',\n",
       " 'computer',\n",
       " 'computers',\n",
       " 'connectivity',\n",
       " 'considerably',\n",
       " 'considered',\n",
       " 'consists',\n",
       " 'containing',\n",
       " 'contest',\n",
       " 'controlled',\n",
       " 'convolution',\n",
       " 'convolutional',\n",
       " 'cooperating',\n",
       " 'coronavirus',\n",
       " 'coronaviruses',\n",
       " 'coughs',\n",
       " 'could',\n",
       " 'covering',\n",
       " 'cpu',\n",
       " 'data',\n",
       " 'de',\n",
       " 'deep',\n",
       " 'demonstrate',\n",
       " 'derivative',\n",
       " 'detection',\n",
       " 'detectors',\n",
       " 'different',\n",
       " 'digital',\n",
       " 'dimension',\n",
       " 'direction',\n",
       " 'directional',\n",
       " 'directly',\n",
       " 'discrete',\n",
       " 'distinguishable',\n",
       " 'distribution',\n",
       " 'doublespending',\n",
       " 'droplets',\n",
       " 'effective',\n",
       " 'efficient',\n",
       " 'effort',\n",
       " 'electronic',\n",
       " 'employed',\n",
       " 'engine',\n",
       " 'entered',\n",
       " 'entities',\n",
       " 'entry',\n",
       " 'error',\n",
       " 'estimate',\n",
       " 'events',\n",
       " 'examines',\n",
       " 'exhaled',\n",
       " 'exploiting',\n",
       " 'face',\n",
       " 'facto',\n",
       " 'factor',\n",
       " 'faster',\n",
       " 'final',\n",
       " 'financial',\n",
       " 'five',\n",
       " 'followed',\n",
       " 'forming',\n",
       " 'found',\n",
       " 'full',\n",
       " 'fullyconnected',\n",
       " 'future',\n",
       " 'gaussian',\n",
       " 'generate',\n",
       " 'generates',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'gpu',\n",
       " 'happened',\n",
       " 'hashbased',\n",
       " 'hashing',\n",
       " 'highefficiency',\n",
       " 'highest',\n",
       " 'highresolution',\n",
       " 'human',\n",
       " 'hypotheses',\n",
       " 'identified',\n",
       " 'illness',\n",
       " 'ilsvrc2012',\n",
       " 'imagenet',\n",
       " 'images',\n",
       " 'implementation',\n",
       " 'indicate',\n",
       " 'indistinguishable',\n",
       " 'individuals',\n",
       " 'influenza',\n",
       " 'institution',\n",
       " 'interferometer',\n",
       " 'intractable',\n",
       " 'investigated',\n",
       " 'jiuzhang',\n",
       " 'large',\n",
       " 'largest',\n",
       " 'layers',\n",
       " 'learning',\n",
       " 'leave',\n",
       " 'long',\n",
       " 'longest',\n",
       " 'lost',\n",
       " 'lsvrc2010',\n",
       " 'machine',\n",
       " 'main',\n",
       " 'majority',\n",
       " 'make',\n",
       " 'masks',\n",
       " 'matrix—the',\n",
       " 'maxpooling',\n",
       " 'messages',\n",
       " 'method',\n",
       " 'metrics',\n",
       " 'million',\n",
       " 'minimal',\n",
       " 'minutes',\n",
       " 'model',\n",
       " 'models',\n",
       " 'movement',\n",
       " 'mse',\n",
       " 'named',\n",
       " 'network',\n",
       " 'neural',\n",
       " 'neurons',\n",
       " 'news',\n",
       " 'nodes',\n",
       " 'nonsaturating',\n",
       " 'noun',\n",
       " 'numeric',\n",
       " 'obtained',\n",
       " 'one',\n",
       " 'ongoing',\n",
       " 'online',\n",
       " 'operation',\n",
       " 'optical',\n",
       " 'outpace',\n",
       " 'output',\n",
       " 'overfitting',\n",
       " 'parameters',\n",
       " 'part',\n",
       " 'party',\n",
       " 'payments',\n",
       " 'peertopeer',\n",
       " 'perform',\n",
       " 'performance',\n",
       " 'performed',\n",
       " 'performs',\n",
       " 'period',\n",
       " 'phaselocked—and',\n",
       " 'photon',\n",
       " 'photonic',\n",
       " 'photons',\n",
       " 'phrases',\n",
       " 'plausible',\n",
       " 'pool',\n",
       " 'power',\n",
       " 'prediction',\n",
       " 'predictive',\n",
       " 'prevent',\n",
       " 'previous',\n",
       " 'price',\n",
       " 'problem',\n",
       " 'promise',\n",
       " 'proof',\n",
       " 'proofofwork',\n",
       " 'proper',\n",
       " 'propose',\n",
       " 'proved',\n",
       " 'provide',\n",
       " 'purely',\n",
       " 'quantum',\n",
       " 'quotes',\n",
       " 'random',\n",
       " 'rate',\n",
       " 'rates',\n",
       " 'recentlydeveloped',\n",
       " 'record',\n",
       " 'redoing',\n",
       " 'reduce',\n",
       " 'reduced',\n",
       " 'regularization',\n",
       " 'rejoin',\n",
       " 'release',\n",
       " 'released',\n",
       " 'representations',\n",
       " 'required',\n",
       " 'requires',\n",
       " 'research',\n",
       " 'respiratory',\n",
       " 'results',\n",
       " 'return',\n",
       " 'rhinoviruses',\n",
       " 'rna',\n",
       " 'samples',\n",
       " 'sampling',\n",
       " 'scheme',\n",
       " 'seasonal',\n",
       " 'secondbest',\n",
       " 'sending',\n",
       " 'sent',\n",
       " 'sequence',\n",
       " 'serves',\n",
       " 'setup',\n",
       " 'several',\n",
       " 'show',\n",
       " 'signatures',\n",
       " 'significantly',\n",
       " 'simulated',\n",
       " 'simulation',\n",
       " 'singlemode',\n",
       " 'singlephoton',\n",
       " 'softmax',\n",
       " 'solution',\n",
       " 'sp',\n",
       " 'specially',\n",
       " 'squeezed',\n",
       " 'standard',\n",
       " 'stateoftheart',\n",
       " 'states',\n",
       " 'statespace',\n",
       " 'still',\n",
       " 'stock',\n",
       " 'stocks',\n",
       " 'stockspecific',\n",
       " 'strategy',\n",
       " 'strong',\n",
       " 'structure',\n",
       " 'supercomputers',\n",
       " 'support',\n",
       " 'surgical',\n",
       " 'svm',\n",
       " 'symptomatic',\n",
       " 'tailored',\n",
       " 'task',\n",
       " 'tasks',\n",
       " 'terms',\n",
       " 'test',\n",
       " 'textual',\n",
       " 'thermal',\n",
       " 'they’ll',\n",
       " 'third',\n",
       " 'three',\n",
       " 'time',\n",
       " 'timestamps',\n",
       " 'top1',\n",
       " 'top5',\n",
       " 'toward',\n",
       " 'trading',\n",
       " 'trained',\n",
       " 'training',\n",
       " 'transactions',\n",
       " 'transmission',\n",
       " 'trend',\n",
       " 'trusted',\n",
       " 'twenty',\n",
       " 'ultralowloss',\n",
       " 'uniform',\n",
       " 'used',\n",
       " 'using',\n",
       " 'validated',\n",
       " 'variables',\n",
       " 'variant',\n",
       " 'vector',\n",
       " 'version',\n",
       " 'virus',\n",
       " 'viruses',\n",
       " 'week',\n",
       " 'whole',\n",
       " 'winning',\n",
       " 'without',\n",
       " 'witnessed',\n",
       " 'words',\n",
       " 'would',\n",
       " 'yields',\n",
       " '“dropout”'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets view the wordSet created using union fuction with uniqe words of abve 6 sentences\n",
    "wordSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c24f1abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create 6 dictionaries to hold the terms(words) in wordSet and the count of those words appear in each and every sentences(documents) considered above\n",
    "#Let's initialise the count of all words to 0 first\n",
    "doc1_wordDict = dict.fromkeys(wordSet, 0) \n",
    "doc2_wordDict = dict.fromkeys(wordSet, 0)\n",
    "doc3_wordDict = dict.fromkeys(wordSet, 0)\n",
    "doc4_wordDict = dict.fromkeys(wordSet, 0)\n",
    "doc5_wordDict = dict.fromkeys(wordSet, 0)\n",
    "query_wordDict = dict.fromkeys(wordSet, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f736bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will get the count of each word appear in each sentence seperately and update the dictionaries respecte to the sentence\n",
    "#eg: for the sentence1 (doc1) we take the count and update doc1_wordDict dictionary.\n",
    "for word in doc1_filtered_sentence:\n",
    "    doc1_wordDict[word]+=1\n",
    "    \n",
    "for word in doc2_filtered_sentence:\n",
    "    doc2_wordDict[word]+=1\n",
    "\n",
    "for word in doc3_filtered_sentence:\n",
    "    doc3_wordDict[word]+=1\n",
    "\n",
    "for word in doc4_filtered_sentence:\n",
    "    doc4_wordDict[word]+=1\n",
    "    \n",
    "for word in doc5_filtered_sentence:\n",
    "    doc5_wordDict[word]+=1\n",
    "    \n",
    "for word in query_filtered_sentence:\n",
    "    query_wordDict[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0b657aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'proof': 0,\n",
       " 'simulated': 1,\n",
       " 'twenty': 1,\n",
       " 'proofofwork': 0,\n",
       " 'cooperating': 0,\n",
       " '1014': 0,\n",
       " 'breath': 0,\n",
       " 'highest': 1,\n",
       " 'rate': 0,\n",
       " 'strategy': 0,\n",
       " 'efficient': 0,\n",
       " '650000': 0,\n",
       " 'investigated': 2,\n",
       " 'majority': 0,\n",
       " 'lost': 0,\n",
       " 'full': 0,\n",
       " 'results': 0,\n",
       " 'performance': 1,\n",
       " 'entities': 1,\n",
       " 'articles': 2,\n",
       " 'output': 0,\n",
       " 'data': 0,\n",
       " 'human': 0,\n",
       " 'bag': 2,\n",
       " 'adults': 0,\n",
       " 'learning': 1,\n",
       " 'changed': 0,\n",
       " 'solution': 0,\n",
       " 'basis': 0,\n",
       " 'news': 3,\n",
       " 'also': 0,\n",
       " 'controlled': 0,\n",
       " 'applied': 1,\n",
       " 'several': 1,\n",
       " 'nonsaturating': 0,\n",
       " 'computer': 0,\n",
       " 'ongoing': 0,\n",
       " 'validated': 0,\n",
       " 'symptomatic': 0,\n",
       " 'trained': 0,\n",
       " 'covering': 1,\n",
       " 'photonic': 0,\n",
       " 'surgical': 0,\n",
       " 'cannot': 0,\n",
       " 'analysis': 2,\n",
       " 'return': 2,\n",
       " 'neurons': 0,\n",
       " 'computers': 0,\n",
       " 'vector': 1,\n",
       " 'hashing': 0,\n",
       " '100': 0,\n",
       " 'broadcast': 0,\n",
       " 'transactions': 0,\n",
       " 'matrix—the': 0,\n",
       " 'phaselocked—and': 0,\n",
       " 'nodes': 0,\n",
       " '170': 0,\n",
       " 'part': 0,\n",
       " 'de': 1,\n",
       " 'cash': 0,\n",
       " '12': 0,\n",
       " 'make': 0,\n",
       " 'face': 0,\n",
       " 'show': 1,\n",
       " 'cpu': 0,\n",
       " 'exhaled': 0,\n",
       " 'discrete': 2,\n",
       " 'droplets': 0,\n",
       " 'transmission': 0,\n",
       " 'timestamps': 0,\n",
       " 'candidate': 0,\n",
       " 'attack': 0,\n",
       " 'rhinoviruses': 0,\n",
       " 'main': 0,\n",
       " 'toward': 0,\n",
       " 'singlemode': 0,\n",
       " 'overfitting': 0,\n",
       " 'consists': 0,\n",
       " 'svm': 1,\n",
       " 'representations': 2,\n",
       " 'obtained': 0,\n",
       " 'individuals': 0,\n",
       " 'numeric': 1,\n",
       " 'peertopeer': 0,\n",
       " 'gone': 0,\n",
       " 'task': 0,\n",
       " 'payments': 0,\n",
       " 'stock': 4,\n",
       " 'requires': 0,\n",
       " '60': 0,\n",
       " 'trend': 0,\n",
       " 'photons': 0,\n",
       " 'using': 3,\n",
       " 'accuracy': 1,\n",
       " 'classes': 0,\n",
       " 'plausible': 0,\n",
       " 'generate': 0,\n",
       " '262': 0,\n",
       " 'sequence': 0,\n",
       " 'squeezed': 0,\n",
       " 'still': 0,\n",
       " 'best': 1,\n",
       " 'sent': 0,\n",
       " 'statespace': 0,\n",
       " 'layers': 0,\n",
       " 'top5': 0,\n",
       " 'regularization': 0,\n",
       " 'largest': 0,\n",
       " 'previous': 0,\n",
       " '1030': 0,\n",
       " 'five': 1,\n",
       " 'financial': 2,\n",
       " 'leave': 0,\n",
       " 'period': 1,\n",
       " 'convolution': 0,\n",
       " 'highefficiency': 0,\n",
       " 'influenza': 0,\n",
       " 'rates': 0,\n",
       " 'operation': 0,\n",
       " 'rna': 0,\n",
       " 'support': 1,\n",
       " 'computational': 0,\n",
       " '100mode': 0,\n",
       " 'version': 0,\n",
       " 'variant': 0,\n",
       " 'purely': 0,\n",
       " '10259042': 1,\n",
       " 'witnessed': 0,\n",
       " 'strong': 0,\n",
       " 'uniform': 0,\n",
       " '9211': 1,\n",
       " 'found': 1,\n",
       " 'illness': 0,\n",
       " 'detectors': 0,\n",
       " 'recentlydeveloped': 0,\n",
       " 'pool': 0,\n",
       " 'employed': 0,\n",
       " 'classify': 0,\n",
       " 'virus': 0,\n",
       " 'training': 0,\n",
       " 'promise': 0,\n",
       " 'serves': 0,\n",
       " 'error': 0,\n",
       " 'random': 0,\n",
       " 'predictive': 1,\n",
       " 'terms': 1,\n",
       " 'method': 0,\n",
       " 'whole': 0,\n",
       " 'thermal': 0,\n",
       " 'would': 0,\n",
       " 'optical': 0,\n",
       " 'used': 0,\n",
       " 'intractable': 0,\n",
       " 'considerably': 0,\n",
       " 'yields': 0,\n",
       " 'secondbest': 0,\n",
       " 'large': 0,\n",
       " 'quantum': 0,\n",
       " 'ilsvrc2012': 0,\n",
       " 'structure': 0,\n",
       " 'aerosols': 0,\n",
       " 'future': 2,\n",
       " 'demonstrate': 0,\n",
       " 'record': 0,\n",
       " 'generates': 0,\n",
       " 'indistinguishable': 0,\n",
       " 'containing': 2,\n",
       " 'variables': 1,\n",
       " 'deep': 0,\n",
       " 'distribution': 0,\n",
       " 'hashbased': 0,\n",
       " 'seasonal': 0,\n",
       " 'exploiting': 0,\n",
       " 'coughs': 0,\n",
       " 'connectivity': 0,\n",
       " 'happened': 0,\n",
       " 'estimate': 1,\n",
       " 'top1': 0,\n",
       " 'supercomputers': 0,\n",
       " 'forming': 0,\n",
       " 'viruses': 0,\n",
       " '1000way': 0,\n",
       " 'scheme': 1,\n",
       " 'coronaviruses': 0,\n",
       " 'rejoin': 0,\n",
       " 'test': 0,\n",
       " 'sp': 1,\n",
       " '153': 0,\n",
       " 'performed': 0,\n",
       " 'clicks': 0,\n",
       " 'simulation': 0,\n",
       " 'week': 1,\n",
       " 'stocks': 1,\n",
       " 'electronic': 0,\n",
       " 'children': 0,\n",
       " 'tailored': 1,\n",
       " 'came': 0,\n",
       " 'stateoftheart': 0,\n",
       " 'believed': 0,\n",
       " 'gpu': 0,\n",
       " 'power': 0,\n",
       " 'proper': 1,\n",
       " 'price': 5,\n",
       " 'photon': 0,\n",
       " 'longest': 0,\n",
       " '1000': 0,\n",
       " 'sampling': 0,\n",
       " 'attackers': 0,\n",
       " 'machine': 2,\n",
       " 'reduced': 0,\n",
       " 'party': 0,\n",
       " 'actual': 1,\n",
       " 'performs': 1,\n",
       " '500': 1,\n",
       " 'proved': 0,\n",
       " 'gaussian': 0,\n",
       " 'convolutional': 0,\n",
       " 'final': 0,\n",
       " 'chain': 0,\n",
       " 'singlephoton': 0,\n",
       " 'directional': 1,\n",
       " 'imagenet': 0,\n",
       " 'directly': 0,\n",
       " 'allow': 0,\n",
       " 'standard': 1,\n",
       " 'compared': 0,\n",
       " 'accepting': 0,\n",
       " '571': 1,\n",
       " 'contest': 0,\n",
       " 'propose': 0,\n",
       " 'benefits': 0,\n",
       " 'lsvrc2010': 0,\n",
       " 'three': 1,\n",
       " 'minutes': 1,\n",
       " 'neural': 0,\n",
       " 'trusted': 0,\n",
       " 'achieved': 0,\n",
       " 'distinguishable': 0,\n",
       " 'mse': 1,\n",
       " 'followed': 0,\n",
       " '375': 0,\n",
       " 'entered': 0,\n",
       " 'they’ll': 0,\n",
       " 'third': 0,\n",
       " 'without': 0,\n",
       " 'long': 0,\n",
       " 'identified': 0,\n",
       " 'perform': 0,\n",
       " 'released': 1,\n",
       " '206': 1,\n",
       " 'ultralowloss': 0,\n",
       " 'sending': 0,\n",
       " 'masks': 0,\n",
       " 'facto': 1,\n",
       " 'time': 1,\n",
       " '004261': 1,\n",
       " 'named': 1,\n",
       " 'approach': 2,\n",
       " 'outpace': 0,\n",
       " 'interferometer': 0,\n",
       " 'messages': 0,\n",
       " 'significantly': 0,\n",
       " 'advantage': 0,\n",
       " 'network': 0,\n",
       " 'stockspecific': 1,\n",
       " 'derivative': 1,\n",
       " 'faster': 0,\n",
       " 'digital': 0,\n",
       " 'direction': 1,\n",
       " 'maxpooling': 0,\n",
       " 'one': 0,\n",
       " 'prediction': 1,\n",
       " 'certain': 0,\n",
       " 'words': 2,\n",
       " 'implementation': 0,\n",
       " 'engine': 1,\n",
       " 'entry': 0,\n",
       " 'boson': 0,\n",
       " 'required': 0,\n",
       " 'million': 0,\n",
       " 'competition': 0,\n",
       " 'considered': 0,\n",
       " 'effort': 0,\n",
       " 'called': 0,\n",
       " 'research': 1,\n",
       " 'quotes': 1,\n",
       " 'closeness': 1,\n",
       " 'setup': 0,\n",
       " 'noun': 2,\n",
       " 'states': 0,\n",
       " 'could': 0,\n",
       " 'going': 0,\n",
       " 'coronavirus': 0,\n",
       " 'classical': 0,\n",
       " 'images': 0,\n",
       " 'online': 0,\n",
       " 'article': 3,\n",
       " 'events': 0,\n",
       " 'model': 1,\n",
       " 'acute': 0,\n",
       " 'textual': 2,\n",
       " 'another': 0,\n",
       " 'specially': 1,\n",
       " 'softmax': 0,\n",
       " 'different': 3,\n",
       " 'examines': 1,\n",
       " 'respiratory': 0,\n",
       " 'phrases': 1,\n",
       " '50': 0,\n",
       " 'effective': 0,\n",
       " 'jiuzhang': 0,\n",
       " 'highresolution': 0,\n",
       " 'doublespending': 0,\n",
       " 'tasks': 0,\n",
       " 'parameters': 0,\n",
       " 'release': 1,\n",
       " 'reduce': 0,\n",
       " 'indicate': 0,\n",
       " 'dimension': 0,\n",
       " 'movement': 1,\n",
       " 'problem': 0,\n",
       " '76': 0,\n",
       " 'hypotheses': 0,\n",
       " 'redoing': 0,\n",
       " 'institution': 0,\n",
       " 'factor': 0,\n",
       " 'fullyconnected': 0,\n",
       " 'better': 1,\n",
       " 'minimal': 0,\n",
       " 'metrics': 1,\n",
       " 'prevent': 0,\n",
       " 'models': 1,\n",
       " 'signatures': 0,\n",
       " 'detection': 0,\n",
       " 'winning': 0,\n",
       " 'provide': 0,\n",
       " 'samples': 0,\n",
       " '“dropout”': 0,\n",
       " 'trading': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1_wordDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba4f8098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proof</th>\n",
       "      <th>simulated</th>\n",
       "      <th>twenty</th>\n",
       "      <th>proofofwork</th>\n",
       "      <th>cooperating</th>\n",
       "      <th>1014</th>\n",
       "      <th>breath</th>\n",
       "      <th>highest</th>\n",
       "      <th>rate</th>\n",
       "      <th>strategy</th>\n",
       "      <th>...</th>\n",
       "      <th>metrics</th>\n",
       "      <th>prevent</th>\n",
       "      <th>models</th>\n",
       "      <th>signatures</th>\n",
       "      <th>detection</th>\n",
       "      <th>winning</th>\n",
       "      <th>provide</th>\n",
       "      <th>samples</th>\n",
       "      <th>“dropout”</th>\n",
       "      <th>trading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 339 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   proof  simulated  twenty  proofofwork  cooperating  1014  breath  highest  \\\n",
       "0      0          1       1            0            0     0       0        1   \n",
       "1      0          0       0            0            0     0       0        0   \n",
       "2      3          0       0            3            1     0       0        0   \n",
       "3      0          0       0            0            0     0       1        0   \n",
       "4      0          0       0            0            0     1       0        0   \n",
       "5      0          0       0            0            0     0       0        0   \n",
       "\n",
       "   rate  strategy  ...  metrics  prevent  models  signatures  detection  \\\n",
       "0     0         0  ...        1        0       1           0          0   \n",
       "1     1         0  ...        0        0       0           0          0   \n",
       "2     0         0  ...        0        1       0           1          0   \n",
       "3     0         0  ...        0        1       0           0          2   \n",
       "4     1         1  ...        0        0       0           0          0   \n",
       "5     0         0  ...        0        0       0           0          0   \n",
       "\n",
       "   winning  provide  samples  “dropout”  trading  \n",
       "0        0        0        0          0        1  \n",
       "1        1        0        0          1        0  \n",
       "2        0        1        0          0        0  \n",
       "3        0        0        0          0        0  \n",
       "4        0        0        1          0        0  \n",
       "5        0        0        0          0        0  \n",
       "\n",
       "[6 rows x 339 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using pandas library create a dataframe(table) using the dictionaries created above to visualise the data with the frequency of each word appear in each sentences\n",
    "pd.DataFrame([doc1_wordDict, doc2_wordDict, doc3_wordDict,doc4_wordDict, doc5_wordDict, query_wordDict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1959b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d359c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copute the TF values for each sentence\n",
    "tfDoc1 = computeTF(doc1_wordDict, doc1_filtered_sentence)\n",
    "tfDoc2 = computeTF(doc2_wordDict, doc2_filtered_sentence)\n",
    "tfDoc3 = computeTF(doc3_wordDict, doc3_filtered_sentence)\n",
    "tfDoc4 = computeTF(doc4_wordDict, doc4_filtered_sentence)\n",
    "tfDoc5 = computeTF(doc5_wordDict, doc5_filtered_sentence)\n",
    "tfQuery = computeTF(query_wordDict, query_filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d576998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'proof': 0.0,\n",
       " 'simulated': 0.008620689655172414,\n",
       " 'twenty': 0.008620689655172414,\n",
       " 'proofofwork': 0.0,\n",
       " 'cooperating': 0.0,\n",
       " '1014': 0.0,\n",
       " 'breath': 0.0,\n",
       " 'highest': 0.008620689655172414,\n",
       " 'rate': 0.0,\n",
       " 'strategy': 0.0,\n",
       " 'efficient': 0.0,\n",
       " '650000': 0.0,\n",
       " 'investigated': 0.017241379310344827,\n",
       " 'majority': 0.0,\n",
       " 'lost': 0.0,\n",
       " 'full': 0.0,\n",
       " 'results': 0.0,\n",
       " 'performance': 0.008620689655172414,\n",
       " 'entities': 0.008620689655172414,\n",
       " 'articles': 0.017241379310344827,\n",
       " 'output': 0.0,\n",
       " 'data': 0.0,\n",
       " 'human': 0.0,\n",
       " 'bag': 0.017241379310344827,\n",
       " 'adults': 0.0,\n",
       " 'learning': 0.008620689655172414,\n",
       " 'changed': 0.0,\n",
       " 'solution': 0.0,\n",
       " 'basis': 0.0,\n",
       " 'news': 0.02586206896551724,\n",
       " 'also': 0.0,\n",
       " 'controlled': 0.0,\n",
       " 'applied': 0.008620689655172414,\n",
       " 'several': 0.008620689655172414,\n",
       " 'nonsaturating': 0.0,\n",
       " 'computer': 0.0,\n",
       " 'ongoing': 0.0,\n",
       " 'validated': 0.0,\n",
       " 'symptomatic': 0.0,\n",
       " 'trained': 0.0,\n",
       " 'covering': 0.008620689655172414,\n",
       " 'photonic': 0.0,\n",
       " 'surgical': 0.0,\n",
       " 'cannot': 0.0,\n",
       " 'analysis': 0.017241379310344827,\n",
       " 'return': 0.017241379310344827,\n",
       " 'neurons': 0.0,\n",
       " 'computers': 0.0,\n",
       " 'vector': 0.008620689655172414,\n",
       " 'hashing': 0.0,\n",
       " '100': 0.0,\n",
       " 'broadcast': 0.0,\n",
       " 'transactions': 0.0,\n",
       " 'matrix—the': 0.0,\n",
       " 'phaselocked—and': 0.0,\n",
       " 'nodes': 0.0,\n",
       " '170': 0.0,\n",
       " 'part': 0.0,\n",
       " 'de': 0.008620689655172414,\n",
       " 'cash': 0.0,\n",
       " '12': 0.0,\n",
       " 'make': 0.0,\n",
       " 'face': 0.0,\n",
       " 'show': 0.008620689655172414,\n",
       " 'cpu': 0.0,\n",
       " 'exhaled': 0.0,\n",
       " 'discrete': 0.017241379310344827,\n",
       " 'droplets': 0.0,\n",
       " 'transmission': 0.0,\n",
       " 'timestamps': 0.0,\n",
       " 'candidate': 0.0,\n",
       " 'attack': 0.0,\n",
       " 'rhinoviruses': 0.0,\n",
       " 'main': 0.0,\n",
       " 'toward': 0.0,\n",
       " 'singlemode': 0.0,\n",
       " 'overfitting': 0.0,\n",
       " 'consists': 0.0,\n",
       " 'svm': 0.008620689655172414,\n",
       " 'representations': 0.017241379310344827,\n",
       " 'obtained': 0.0,\n",
       " 'individuals': 0.0,\n",
       " 'numeric': 0.008620689655172414,\n",
       " 'peertopeer': 0.0,\n",
       " 'gone': 0.0,\n",
       " 'task': 0.0,\n",
       " 'payments': 0.0,\n",
       " 'stock': 0.034482758620689655,\n",
       " 'requires': 0.0,\n",
       " '60': 0.0,\n",
       " 'trend': 0.0,\n",
       " 'photons': 0.0,\n",
       " 'using': 0.02586206896551724,\n",
       " 'accuracy': 0.008620689655172414,\n",
       " 'classes': 0.0,\n",
       " 'plausible': 0.0,\n",
       " 'generate': 0.0,\n",
       " '262': 0.0,\n",
       " 'sequence': 0.0,\n",
       " 'squeezed': 0.0,\n",
       " 'still': 0.0,\n",
       " 'best': 0.008620689655172414,\n",
       " 'sent': 0.0,\n",
       " 'statespace': 0.0,\n",
       " 'layers': 0.0,\n",
       " 'top5': 0.0,\n",
       " 'regularization': 0.0,\n",
       " 'largest': 0.0,\n",
       " 'previous': 0.0,\n",
       " '1030': 0.0,\n",
       " 'five': 0.008620689655172414,\n",
       " 'financial': 0.017241379310344827,\n",
       " 'leave': 0.0,\n",
       " 'period': 0.008620689655172414,\n",
       " 'convolution': 0.0,\n",
       " 'highefficiency': 0.0,\n",
       " 'influenza': 0.0,\n",
       " 'rates': 0.0,\n",
       " 'operation': 0.0,\n",
       " 'rna': 0.0,\n",
       " 'support': 0.008620689655172414,\n",
       " 'computational': 0.0,\n",
       " '100mode': 0.0,\n",
       " 'version': 0.0,\n",
       " 'variant': 0.0,\n",
       " 'purely': 0.0,\n",
       " '10259042': 0.008620689655172414,\n",
       " 'witnessed': 0.0,\n",
       " 'strong': 0.0,\n",
       " 'uniform': 0.0,\n",
       " '9211': 0.008620689655172414,\n",
       " 'found': 0.008620689655172414,\n",
       " 'illness': 0.0,\n",
       " 'detectors': 0.0,\n",
       " 'recentlydeveloped': 0.0,\n",
       " 'pool': 0.0,\n",
       " 'employed': 0.0,\n",
       " 'classify': 0.0,\n",
       " 'virus': 0.0,\n",
       " 'training': 0.0,\n",
       " 'promise': 0.0,\n",
       " 'serves': 0.0,\n",
       " 'error': 0.0,\n",
       " 'random': 0.0,\n",
       " 'predictive': 0.008620689655172414,\n",
       " 'terms': 0.008620689655172414,\n",
       " 'method': 0.0,\n",
       " 'whole': 0.0,\n",
       " 'thermal': 0.0,\n",
       " 'would': 0.0,\n",
       " 'optical': 0.0,\n",
       " 'used': 0.0,\n",
       " 'intractable': 0.0,\n",
       " 'considerably': 0.0,\n",
       " 'yields': 0.0,\n",
       " 'secondbest': 0.0,\n",
       " 'large': 0.0,\n",
       " 'quantum': 0.0,\n",
       " 'ilsvrc2012': 0.0,\n",
       " 'structure': 0.0,\n",
       " 'aerosols': 0.0,\n",
       " 'future': 0.017241379310344827,\n",
       " 'demonstrate': 0.0,\n",
       " 'record': 0.0,\n",
       " 'generates': 0.0,\n",
       " 'indistinguishable': 0.0,\n",
       " 'containing': 0.017241379310344827,\n",
       " 'variables': 0.008620689655172414,\n",
       " 'deep': 0.0,\n",
       " 'distribution': 0.0,\n",
       " 'hashbased': 0.0,\n",
       " 'seasonal': 0.0,\n",
       " 'exploiting': 0.0,\n",
       " 'coughs': 0.0,\n",
       " 'connectivity': 0.0,\n",
       " 'happened': 0.0,\n",
       " 'estimate': 0.008620689655172414,\n",
       " 'top1': 0.0,\n",
       " 'supercomputers': 0.0,\n",
       " 'forming': 0.0,\n",
       " 'viruses': 0.0,\n",
       " '1000way': 0.0,\n",
       " 'scheme': 0.008620689655172414,\n",
       " 'coronaviruses': 0.0,\n",
       " 'rejoin': 0.0,\n",
       " 'test': 0.0,\n",
       " 'sp': 0.008620689655172414,\n",
       " '153': 0.0,\n",
       " 'performed': 0.0,\n",
       " 'clicks': 0.0,\n",
       " 'simulation': 0.0,\n",
       " 'week': 0.008620689655172414,\n",
       " 'stocks': 0.008620689655172414,\n",
       " 'electronic': 0.0,\n",
       " 'children': 0.0,\n",
       " 'tailored': 0.008620689655172414,\n",
       " 'came': 0.0,\n",
       " 'stateoftheart': 0.0,\n",
       " 'believed': 0.0,\n",
       " 'gpu': 0.0,\n",
       " 'power': 0.0,\n",
       " 'proper': 0.008620689655172414,\n",
       " 'price': 0.04310344827586207,\n",
       " 'photon': 0.0,\n",
       " 'longest': 0.0,\n",
       " '1000': 0.0,\n",
       " 'sampling': 0.0,\n",
       " 'attackers': 0.0,\n",
       " 'machine': 0.017241379310344827,\n",
       " 'reduced': 0.0,\n",
       " 'party': 0.0,\n",
       " 'actual': 0.008620689655172414,\n",
       " 'performs': 0.008620689655172414,\n",
       " '500': 0.008620689655172414,\n",
       " 'proved': 0.0,\n",
       " 'gaussian': 0.0,\n",
       " 'convolutional': 0.0,\n",
       " 'final': 0.0,\n",
       " 'chain': 0.0,\n",
       " 'singlephoton': 0.0,\n",
       " 'directional': 0.008620689655172414,\n",
       " 'imagenet': 0.0,\n",
       " 'directly': 0.0,\n",
       " 'allow': 0.0,\n",
       " 'standard': 0.008620689655172414,\n",
       " 'compared': 0.0,\n",
       " 'accepting': 0.0,\n",
       " '571': 0.008620689655172414,\n",
       " 'contest': 0.0,\n",
       " 'propose': 0.0,\n",
       " 'benefits': 0.0,\n",
       " 'lsvrc2010': 0.0,\n",
       " 'three': 0.008620689655172414,\n",
       " 'minutes': 0.008620689655172414,\n",
       " 'neural': 0.0,\n",
       " 'trusted': 0.0,\n",
       " 'achieved': 0.0,\n",
       " 'distinguishable': 0.0,\n",
       " 'mse': 0.008620689655172414,\n",
       " 'followed': 0.0,\n",
       " '375': 0.0,\n",
       " 'entered': 0.0,\n",
       " 'they’ll': 0.0,\n",
       " 'third': 0.0,\n",
       " 'without': 0.0,\n",
       " 'long': 0.0,\n",
       " 'identified': 0.0,\n",
       " 'perform': 0.0,\n",
       " 'released': 0.008620689655172414,\n",
       " '206': 0.008620689655172414,\n",
       " 'ultralowloss': 0.0,\n",
       " 'sending': 0.0,\n",
       " 'masks': 0.0,\n",
       " 'facto': 0.008620689655172414,\n",
       " 'time': 0.008620689655172414,\n",
       " '004261': 0.008620689655172414,\n",
       " 'named': 0.008620689655172414,\n",
       " 'approach': 0.017241379310344827,\n",
       " 'outpace': 0.0,\n",
       " 'interferometer': 0.0,\n",
       " 'messages': 0.0,\n",
       " 'significantly': 0.0,\n",
       " 'advantage': 0.0,\n",
       " 'network': 0.0,\n",
       " 'stockspecific': 0.008620689655172414,\n",
       " 'derivative': 0.008620689655172414,\n",
       " 'faster': 0.0,\n",
       " 'digital': 0.0,\n",
       " 'direction': 0.008620689655172414,\n",
       " 'maxpooling': 0.0,\n",
       " 'one': 0.0,\n",
       " 'prediction': 0.008620689655172414,\n",
       " 'certain': 0.0,\n",
       " 'words': 0.017241379310344827,\n",
       " 'implementation': 0.0,\n",
       " 'engine': 0.008620689655172414,\n",
       " 'entry': 0.0,\n",
       " 'boson': 0.0,\n",
       " 'required': 0.0,\n",
       " 'million': 0.0,\n",
       " 'competition': 0.0,\n",
       " 'considered': 0.0,\n",
       " 'effort': 0.0,\n",
       " 'called': 0.0,\n",
       " 'research': 0.008620689655172414,\n",
       " 'quotes': 0.008620689655172414,\n",
       " 'closeness': 0.008620689655172414,\n",
       " 'setup': 0.0,\n",
       " 'noun': 0.017241379310344827,\n",
       " 'states': 0.0,\n",
       " 'could': 0.0,\n",
       " 'going': 0.0,\n",
       " 'coronavirus': 0.0,\n",
       " 'classical': 0.0,\n",
       " 'images': 0.0,\n",
       " 'online': 0.0,\n",
       " 'article': 0.02586206896551724,\n",
       " 'events': 0.0,\n",
       " 'model': 0.008620689655172414,\n",
       " 'acute': 0.0,\n",
       " 'textual': 0.017241379310344827,\n",
       " 'another': 0.0,\n",
       " 'specially': 0.008620689655172414,\n",
       " 'softmax': 0.0,\n",
       " 'different': 0.02586206896551724,\n",
       " 'examines': 0.008620689655172414,\n",
       " 'respiratory': 0.0,\n",
       " 'phrases': 0.008620689655172414,\n",
       " '50': 0.0,\n",
       " 'effective': 0.0,\n",
       " 'jiuzhang': 0.0,\n",
       " 'highresolution': 0.0,\n",
       " 'doublespending': 0.0,\n",
       " 'tasks': 0.0,\n",
       " 'parameters': 0.0,\n",
       " 'release': 0.008620689655172414,\n",
       " 'reduce': 0.0,\n",
       " 'indicate': 0.0,\n",
       " 'dimension': 0.0,\n",
       " 'movement': 0.008620689655172414,\n",
       " 'problem': 0.0,\n",
       " '76': 0.0,\n",
       " 'hypotheses': 0.0,\n",
       " 'redoing': 0.0,\n",
       " 'institution': 0.0,\n",
       " 'factor': 0.0,\n",
       " 'fullyconnected': 0.0,\n",
       " 'better': 0.008620689655172414,\n",
       " 'minimal': 0.0,\n",
       " 'metrics': 0.008620689655172414,\n",
       " 'prevent': 0.0,\n",
       " 'models': 0.008620689655172414,\n",
       " 'signatures': 0.0,\n",
       " 'detection': 0.0,\n",
       " 'winning': 0.0,\n",
       " 'provide': 0.0,\n",
       " 'samples': 0.0,\n",
       " '“dropout”': 0.0,\n",
       " 'trading': 0.008620689655172414}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfDoc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acea93bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to calculate the Inverse Document Frequency\n",
    "def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e42fa1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets use the computeIDF function to compute IDF\n",
    "idfs = computeIDF([doc1_wordDict, doc2_wordDict, doc3_wordDict, doc4_wordDict, doc5_wordDict, query_wordDict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "649c78e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'proof': 0.7781512503836436,\n",
       " 'simulated': 0.7781512503836436,\n",
       " 'twenty': 0.7781512503836436,\n",
       " 'proofofwork': 0.7781512503836436,\n",
       " 'cooperating': 0.7781512503836436,\n",
       " '1014': 0.7781512503836436,\n",
       " 'breath': 0.7781512503836436,\n",
       " 'highest': 0.7781512503836436,\n",
       " 'rate': 0.47712125471966244,\n",
       " 'strategy': 0.7781512503836436,\n",
       " 'efficient': 0.7781512503836436,\n",
       " '650000': 0.47712125471966244,\n",
       " 'investigated': 0.7781512503836436,\n",
       " 'majority': 0.7781512503836436,\n",
       " 'lost': 0.7781512503836436,\n",
       " 'full': 0.7781512503836436,\n",
       " 'results': 0.7781512503836436,\n",
       " 'performance': 0.7781512503836436,\n",
       " 'entities': 0.7781512503836436,\n",
       " 'articles': 0.7781512503836436,\n",
       " 'output': 0.7781512503836436,\n",
       " 'data': 0.7781512503836436,\n",
       " 'human': 0.7781512503836436,\n",
       " 'bag': 0.7781512503836436,\n",
       " 'adults': 0.7781512503836436,\n",
       " 'learning': 0.7781512503836436,\n",
       " 'changed': 0.7781512503836436,\n",
       " 'solution': 0.7781512503836436,\n",
       " 'basis': 0.7781512503836436,\n",
       " 'news': 0.7781512503836436,\n",
       " 'also': 0.7781512503836436,\n",
       " 'controlled': 0.7781512503836436,\n",
       " 'applied': 0.7781512503836436,\n",
       " 'several': 0.7781512503836436,\n",
       " 'nonsaturating': 0.7781512503836436,\n",
       " 'computer': 0.7781512503836436,\n",
       " 'ongoing': 0.7781512503836436,\n",
       " 'validated': 0.7781512503836436,\n",
       " 'symptomatic': 0.7781512503836436,\n",
       " 'trained': 0.47712125471966244,\n",
       " 'covering': 0.7781512503836436,\n",
       " 'photonic': 0.7781512503836436,\n",
       " 'surgical': 0.7781512503836436,\n",
       " 'cannot': 0.7781512503836436,\n",
       " 'analysis': 0.7781512503836436,\n",
       " 'return': 0.7781512503836436,\n",
       " 'neurons': 0.47712125471966244,\n",
       " 'computers': 0.7781512503836436,\n",
       " 'vector': 0.7781512503836436,\n",
       " 'hashing': 0.7781512503836436,\n",
       " '100': 0.7781512503836436,\n",
       " 'broadcast': 0.7781512503836436,\n",
       " 'transactions': 0.7781512503836436,\n",
       " 'matrix—the': 0.7781512503836436,\n",
       " 'phaselocked—and': 0.7781512503836436,\n",
       " 'nodes': 0.7781512503836436,\n",
       " '170': 0.7781512503836436,\n",
       " 'part': 0.7781512503836436,\n",
       " 'de': 0.7781512503836436,\n",
       " 'cash': 0.7781512503836436,\n",
       " '12': 0.47712125471966244,\n",
       " 'make': 0.7781512503836436,\n",
       " 'face': 0.7781512503836436,\n",
       " 'show': 0.7781512503836436,\n",
       " 'cpu': 0.7781512503836436,\n",
       " 'exhaled': 0.7781512503836436,\n",
       " 'discrete': 0.7781512503836436,\n",
       " 'droplets': 0.7781512503836436,\n",
       " 'transmission': 0.7781512503836436,\n",
       " 'timestamps': 0.7781512503836436,\n",
       " 'candidate': 0.7781512503836436,\n",
       " 'attack': 0.7781512503836436,\n",
       " 'rhinoviruses': 0.7781512503836436,\n",
       " 'main': 0.7781512503836436,\n",
       " 'toward': 0.7781512503836436,\n",
       " 'singlemode': 0.7781512503836436,\n",
       " 'overfitting': 0.7781512503836436,\n",
       " 'consists': 0.47712125471966244,\n",
       " 'svm': 0.7781512503836436,\n",
       " 'representations': 0.7781512503836436,\n",
       " 'obtained': 0.7781512503836436,\n",
       " 'individuals': 0.7781512503836436,\n",
       " 'numeric': 0.7781512503836436,\n",
       " 'peertopeer': 0.7781512503836436,\n",
       " 'gone': 0.7781512503836436,\n",
       " 'task': 0.7781512503836436,\n",
       " 'payments': 0.7781512503836436,\n",
       " 'stock': 0.7781512503836436,\n",
       " 'requires': 0.7781512503836436,\n",
       " '60': 0.47712125471966244,\n",
       " 'trend': 0.7781512503836436,\n",
       " 'photons': 0.7781512503836436,\n",
       " 'using': 0.3010299956639812,\n",
       " 'accuracy': 0.7781512503836436,\n",
       " 'classes': 0.47712125471966244,\n",
       " 'plausible': 0.7781512503836436,\n",
       " 'generate': 0.7781512503836436,\n",
       " '262': 0.7781512503836436,\n",
       " 'sequence': 0.7781512503836436,\n",
       " 'squeezed': 0.7781512503836436,\n",
       " 'still': 0.7781512503836436,\n",
       " 'best': 0.47712125471966244,\n",
       " 'sent': 0.7781512503836436,\n",
       " 'statespace': 0.7781512503836436,\n",
       " 'layers': 0.47712125471966244,\n",
       " 'top5': 0.7781512503836436,\n",
       " 'regularization': 0.7781512503836436,\n",
       " 'largest': 0.7781512503836436,\n",
       " 'previous': 0.7781512503836436,\n",
       " '1030': 0.7781512503836436,\n",
       " 'five': 0.3010299956639812,\n",
       " 'financial': 0.47712125471966244,\n",
       " 'leave': 0.7781512503836436,\n",
       " 'period': 0.7781512503836436,\n",
       " 'convolution': 0.7781512503836436,\n",
       " 'highefficiency': 0.7781512503836436,\n",
       " 'influenza': 0.7781512503836436,\n",
       " 'rates': 0.7781512503836436,\n",
       " 'operation': 0.7781512503836436,\n",
       " 'rna': 0.7781512503836436,\n",
       " 'support': 0.7781512503836436,\n",
       " 'computational': 0.7781512503836436,\n",
       " '100mode': 0.7781512503836436,\n",
       " 'version': 0.7781512503836436,\n",
       " 'variant': 0.7781512503836436,\n",
       " 'purely': 0.7781512503836436,\n",
       " '10259042': 0.7781512503836436,\n",
       " 'witnessed': 0.7781512503836436,\n",
       " 'strong': 0.7781512503836436,\n",
       " 'uniform': 0.7781512503836436,\n",
       " '9211': 0.7781512503836436,\n",
       " 'found': 0.7781512503836436,\n",
       " 'illness': 0.7781512503836436,\n",
       " 'detectors': 0.7781512503836436,\n",
       " 'recentlydeveloped': 0.7781512503836436,\n",
       " 'pool': 0.7781512503836436,\n",
       " 'employed': 0.7781512503836436,\n",
       " 'classify': 0.47712125471966244,\n",
       " 'virus': 0.7781512503836436,\n",
       " 'training': 0.7781512503836436,\n",
       " 'promise': 0.7781512503836436,\n",
       " 'serves': 0.7781512503836436,\n",
       " 'error': 0.7781512503836436,\n",
       " 'random': 0.7781512503836436,\n",
       " 'predictive': 0.7781512503836436,\n",
       " 'terms': 0.7781512503836436,\n",
       " 'method': 0.7781512503836436,\n",
       " 'whole': 0.7781512503836436,\n",
       " 'thermal': 0.7781512503836436,\n",
       " 'would': 0.7781512503836436,\n",
       " 'optical': 0.7781512503836436,\n",
       " 'used': 0.7781512503836436,\n",
       " 'intractable': 0.7781512503836436,\n",
       " 'considerably': 0.7781512503836436,\n",
       " 'yields': 0.7781512503836436,\n",
       " 'secondbest': 0.7781512503836436,\n",
       " 'large': 0.7781512503836436,\n",
       " 'quantum': 0.7781512503836436,\n",
       " 'ilsvrc2012': 0.7781512503836436,\n",
       " 'structure': 0.7781512503836436,\n",
       " 'aerosols': 0.7781512503836436,\n",
       " 'future': 0.7781512503836436,\n",
       " 'demonstrate': 0.7781512503836436,\n",
       " 'record': 0.7781512503836436,\n",
       " 'generates': 0.7781512503836436,\n",
       " 'indistinguishable': 0.7781512503836436,\n",
       " 'containing': 0.7781512503836436,\n",
       " 'variables': 0.7781512503836436,\n",
       " 'deep': 0.47712125471966244,\n",
       " 'distribution': 0.7781512503836436,\n",
       " 'hashbased': 0.7781512503836436,\n",
       " 'seasonal': 0.7781512503836436,\n",
       " 'exploiting': 0.7781512503836436,\n",
       " 'coughs': 0.7781512503836436,\n",
       " 'connectivity': 0.7781512503836436,\n",
       " 'happened': 0.7781512503836436,\n",
       " 'estimate': 0.7781512503836436,\n",
       " 'top1': 0.7781512503836436,\n",
       " 'supercomputers': 0.7781512503836436,\n",
       " 'forming': 0.7781512503836436,\n",
       " 'viruses': 0.7781512503836436,\n",
       " '1000way': 0.47712125471966244,\n",
       " 'scheme': 0.7781512503836436,\n",
       " 'coronaviruses': 0.7781512503836436,\n",
       " 'rejoin': 0.7781512503836436,\n",
       " 'test': 0.7781512503836436,\n",
       " 'sp': 0.7781512503836436,\n",
       " '153': 0.7781512503836436,\n",
       " 'performed': 0.7781512503836436,\n",
       " 'clicks': 0.7781512503836436,\n",
       " 'simulation': 0.7781512503836436,\n",
       " 'week': 0.7781512503836436,\n",
       " 'stocks': 0.7781512503836436,\n",
       " 'electronic': 0.7781512503836436,\n",
       " 'children': 0.7781512503836436,\n",
       " 'tailored': 0.7781512503836436,\n",
       " 'came': 0.7781512503836436,\n",
       " 'stateoftheart': 0.47712125471966244,\n",
       " 'believed': 0.7781512503836436,\n",
       " 'gpu': 0.7781512503836436,\n",
       " 'power': 0.7781512503836436,\n",
       " 'proper': 0.7781512503836436,\n",
       " 'price': 0.7781512503836436,\n",
       " 'photon': 0.7781512503836436,\n",
       " 'longest': 0.7781512503836436,\n",
       " '1000': 0.47712125471966244,\n",
       " 'sampling': 0.7781512503836436,\n",
       " 'attackers': 0.7781512503836436,\n",
       " 'machine': 0.7781512503836436,\n",
       " 'reduced': 0.7781512503836436,\n",
       " 'party': 0.7781512503836436,\n",
       " 'actual': 0.7781512503836436,\n",
       " 'performs': 0.7781512503836436,\n",
       " '500': 0.7781512503836436,\n",
       " 'proved': 0.7781512503836436,\n",
       " 'gaussian': 0.7781512503836436,\n",
       " 'convolutional': 0.47712125471966244,\n",
       " 'final': 0.47712125471966244,\n",
       " 'chain': 0.7781512503836436,\n",
       " 'singlephoton': 0.7781512503836436,\n",
       " 'directional': 0.7781512503836436,\n",
       " 'imagenet': 0.47712125471966244,\n",
       " 'directly': 0.7781512503836436,\n",
       " 'allow': 0.7781512503836436,\n",
       " 'standard': 0.7781512503836436,\n",
       " 'compared': 0.7781512503836436,\n",
       " 'accepting': 0.7781512503836436,\n",
       " '571': 0.7781512503836436,\n",
       " 'contest': 0.47712125471966244,\n",
       " 'propose': 0.7781512503836436,\n",
       " 'benefits': 0.7781512503836436,\n",
       " 'lsvrc2010': 0.47712125471966244,\n",
       " 'three': 0.3010299956639812,\n",
       " 'minutes': 0.7781512503836436,\n",
       " 'neural': 0.47712125471966244,\n",
       " 'trusted': 0.7781512503836436,\n",
       " 'achieved': 0.7781512503836436,\n",
       " 'distinguishable': 0.7781512503836436,\n",
       " 'mse': 0.7781512503836436,\n",
       " 'followed': 0.47712125471966244,\n",
       " '375': 0.7781512503836436,\n",
       " 'entered': 0.7781512503836436,\n",
       " 'they’ll': 0.7781512503836436,\n",
       " 'third': 0.7781512503836436,\n",
       " 'without': 0.7781512503836436,\n",
       " 'long': 0.7781512503836436,\n",
       " 'identified': 0.7781512503836436,\n",
       " 'perform': 0.7781512503836436,\n",
       " 'released': 0.7781512503836436,\n",
       " '206': 0.7781512503836436,\n",
       " 'ultralowloss': 0.7781512503836436,\n",
       " 'sending': 0.7781512503836436,\n",
       " 'masks': 0.7781512503836436,\n",
       " 'facto': 0.7781512503836436,\n",
       " 'time': 0.7781512503836436,\n",
       " '004261': 0.7781512503836436,\n",
       " 'named': 0.7781512503836436,\n",
       " 'approach': 0.7781512503836436,\n",
       " 'outpace': 0.7781512503836436,\n",
       " 'interferometer': 0.7781512503836436,\n",
       " 'messages': 0.7781512503836436,\n",
       " 'significantly': 0.7781512503836436,\n",
       " 'advantage': 0.7781512503836436,\n",
       " 'network': 0.3010299956639812,\n",
       " 'stockspecific': 0.7781512503836436,\n",
       " 'derivative': 0.7781512503836436,\n",
       " 'faster': 0.47712125471966244,\n",
       " 'digital': 0.7781512503836436,\n",
       " 'direction': 0.7781512503836436,\n",
       " 'maxpooling': 0.47712125471966244,\n",
       " 'one': 0.7781512503836436,\n",
       " 'prediction': 0.7781512503836436,\n",
       " 'certain': 0.7781512503836436,\n",
       " 'words': 0.7781512503836436,\n",
       " 'implementation': 0.7781512503836436,\n",
       " 'engine': 0.7781512503836436,\n",
       " 'entry': 0.7781512503836436,\n",
       " 'boson': 0.7781512503836436,\n",
       " 'required': 0.7781512503836436,\n",
       " 'million': 0.47712125471966244,\n",
       " 'competition': 0.7781512503836436,\n",
       " 'considered': 0.7781512503836436,\n",
       " 'effort': 0.7781512503836436,\n",
       " 'called': 0.7781512503836436,\n",
       " 'research': 0.7781512503836436,\n",
       " 'quotes': 0.7781512503836436,\n",
       " 'closeness': 0.7781512503836436,\n",
       " 'setup': 0.7781512503836436,\n",
       " 'noun': 0.7781512503836436,\n",
       " 'states': 0.7781512503836436,\n",
       " 'could': 0.7781512503836436,\n",
       " 'going': 0.7781512503836436,\n",
       " 'coronavirus': 0.7781512503836436,\n",
       " 'classical': 0.7781512503836436,\n",
       " 'images': 0.47712125471966244,\n",
       " 'online': 0.7781512503836436,\n",
       " 'article': 0.7781512503836436,\n",
       " 'events': 0.7781512503836436,\n",
       " 'model': 0.47712125471966244,\n",
       " 'acute': 0.7781512503836436,\n",
       " 'textual': 0.7781512503836436,\n",
       " 'another': 0.7781512503836436,\n",
       " 'specially': 0.7781512503836436,\n",
       " 'softmax': 0.47712125471966244,\n",
       " 'different': 0.3010299956639812,\n",
       " 'examines': 0.7781512503836436,\n",
       " 'respiratory': 0.7781512503836436,\n",
       " 'phrases': 0.7781512503836436,\n",
       " '50': 0.7781512503836436,\n",
       " 'effective': 0.7781512503836436,\n",
       " 'jiuzhang': 0.7781512503836436,\n",
       " 'highresolution': 0.47712125471966244,\n",
       " 'doublespending': 0.7781512503836436,\n",
       " 'tasks': 0.7781512503836436,\n",
       " 'parameters': 0.47712125471966244,\n",
       " 'release': 0.7781512503836436,\n",
       " 'reduce': 0.7781512503836436,\n",
       " 'indicate': 0.7781512503836436,\n",
       " 'dimension': 0.7781512503836436,\n",
       " 'movement': 0.7781512503836436,\n",
       " 'problem': 0.7781512503836436,\n",
       " '76': 0.7781512503836436,\n",
       " 'hypotheses': 0.7781512503836436,\n",
       " 'redoing': 0.7781512503836436,\n",
       " 'institution': 0.7781512503836436,\n",
       " 'factor': 0.7781512503836436,\n",
       " 'fullyconnected': 0.47712125471966244,\n",
       " 'better': 0.47712125471966244,\n",
       " 'minimal': 0.7781512503836436,\n",
       " 'metrics': 0.7781512503836436,\n",
       " 'prevent': 0.47712125471966244,\n",
       " 'models': 0.7781512503836436,\n",
       " 'signatures': 0.7781512503836436,\n",
       " 'detection': 0.7781512503836436,\n",
       " 'winning': 0.7781512503836436,\n",
       " 'provide': 0.7781512503836436,\n",
       " 'samples': 0.7781512503836436,\n",
       " '“dropout”': 0.7781512503836436,\n",
       " 'trading': 0.7781512503836436}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96f5ec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will calculate the TF*IDF value in a dictionary format\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3485bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfDoc1 = computeTFIDF(tfDoc1, idfs)\n",
    "tfidfDoc2 = computeTFIDF(tfDoc2, idfs)\n",
    "tfidfDoc3 = computeTFIDF(tfDoc3, idfs)\n",
    "tfidfDoc4 = computeTFIDF(tfDoc4, idfs)\n",
    "tfidfDoc5 = computeTFIDF(tfDoc5, idfs)\n",
    "tfidfQuery = computeTFIDF(tfQuery, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26de1753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proof</th>\n",
       "      <th>simulated</th>\n",
       "      <th>twenty</th>\n",
       "      <th>proofofwork</th>\n",
       "      <th>cooperating</th>\n",
       "      <th>1014</th>\n",
       "      <th>breath</th>\n",
       "      <th>highest</th>\n",
       "      <th>rate</th>\n",
       "      <th>strategy</th>\n",
       "      <th>...</th>\n",
       "      <th>metrics</th>\n",
       "      <th>prevent</th>\n",
       "      <th>models</th>\n",
       "      <th>signatures</th>\n",
       "      <th>detection</th>\n",
       "      <th>winning</th>\n",
       "      <th>provide</th>\n",
       "      <th>samples</th>\n",
       "      <th>“dropout”</th>\n",
       "      <th>trading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006708</td>\n",
       "      <td>0.006708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008551</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022665</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022665</td>\n",
       "      <td>0.007555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005680</td>\n",
       "      <td>0.009264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 339 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      proof  simulated    twenty  proofofwork  cooperating      1014  \\\n",
       "0  0.000000   0.006708  0.006708     0.000000     0.000000  0.000000   \n",
       "1  0.000000   0.000000  0.000000     0.000000     0.000000  0.000000   \n",
       "2  0.022665   0.000000  0.000000     0.022665     0.007555  0.000000   \n",
       "3  0.000000   0.000000  0.000000     0.000000     0.000000  0.000000   \n",
       "4  0.000000   0.000000  0.000000     0.000000     0.000000  0.009264   \n",
       "5  0.000000   0.000000  0.000000     0.000000     0.000000  0.000000   \n",
       "\n",
       "     breath   highest      rate  strategy  ...   metrics   prevent    models  \\\n",
       "0  0.000000  0.006708  0.000000  0.000000  ...  0.006708  0.000000  0.006708   \n",
       "1  0.000000  0.000000  0.005243  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.004632  0.000000   \n",
       "3  0.015258  0.000000  0.000000  0.000000  ...  0.000000  0.009355  0.000000   \n",
       "4  0.000000  0.000000  0.005680  0.009264  ...  0.000000  0.000000  0.000000   \n",
       "5  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "   signatures  detection   winning   provide   samples  “dropout”   trading  \n",
       "0    0.000000   0.000000  0.000000  0.000000  0.000000   0.000000  0.006708  \n",
       "1    0.000000   0.000000  0.008551  0.000000  0.000000   0.008551  0.000000  \n",
       "2    0.007555   0.000000  0.000000  0.007555  0.000000   0.000000  0.000000  \n",
       "3    0.000000   0.030516  0.000000  0.000000  0.000000   0.000000  0.000000  \n",
       "4    0.000000   0.000000  0.000000  0.000000  0.009264   0.000000  0.000000  \n",
       "5    0.000000   0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  \n",
       "\n",
       "[6 rows x 339 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([tfidfDoc1, tfidfDoc2, tfidfDoc3, tfidfDoc4, tfidfDoc5, tfidfQuery])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89fe0b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.006708200434341755, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.01341640086868351, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.006708200434341755, 0.01341640086868351, 0.0, 0.0, 0.0, 0.01341640086868351, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.020124601303025267, 0.0, 0.0, 0.006708200434341755, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.01341640086868351, 0.01341640086868351, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.01341640086868351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.01341640086868351, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.02683280173736702, 0.0, 0.0, 0.0, 0.0, 0.007785258508551237, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004113114264824676, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002595086169517079, 0.008226228529649353, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.006708200434341755, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01341640086868351, 0.0, 0.0, 0.0, 0.0, 0.01341640086868351, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.006708200434341755, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.03354100217170878, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01341640086868351, 0.0, 0.0, 0.006708200434341755, 0.006708200434341755, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.002595086169517079, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.006708200434341755, 0.0, 0.0, 0.0, 0.006708200434341755, 0.006708200434341755, 0.006708200434341755, 0.006708200434341755, 0.01341640086868351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.006708200434341755, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.006708200434341755, 0.0, 0.01341640086868351, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.006708200434341755, 0.006708200434341755, 0.0, 0.01341640086868351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020124601303025267, 0.0, 0.004113114264824676, 0.0, 0.01341640086868351, 0.0, 0.006708200434341755, 0.0, 0.007785258508551237, 0.006708200434341755, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004113114264824676, 0.0, 0.006708200434341755, 0.0, 0.006708200434341755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006708200434341755]\n"
     ]
    }
   ],
   "source": [
    "print (list(tfidfDoc1.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4469a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that spatial.distance.cosine computes the distance, and not the similarity. So, you must subtract the value from 1 to get the similarity.\n",
    "#result = 1 - spatial.distance.cosine(dataSetI, dataSetII)\n",
    "doc1_result = 1 - spatial.distance.cosine(list(tfidfDoc1.values()),list(tfidfQuery.values()))\n",
    "doc2_result = 1 - spatial.distance.cosine(list(tfidfDoc2.values()),list(tfidfQuery.values()))\n",
    "doc3_result = 1 - spatial.distance.cosine(list(tfidfDoc3.values()),list(tfidfQuery.values()))\n",
    "doc4_result = 1 - spatial.distance.cosine(list(tfidfDoc4.values()),list(tfidfQuery.values()))\n",
    "doc5_result = 1 - spatial.distance.cosine(list(tfidfDoc5.values()),list(tfidfQuery.values()))\n",
    "query_result = 1 - spatial.distance.cosine(list(tfidfQuery.values()),list(tfidfQuery.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "163bd59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014079969852302754 0.5008691936023256 0.015713686254851922 0.0 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print(doc1_result, doc2_result, doc3_result,doc4_result, doc5_result, query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eadbe3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document 2 is the most similar document to Query document"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
